\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[legalpaper, margin=1in]{geometry}
\setlength{\parindent}{0pt}

% Formatting commands
\newcommand{\n}{\hfill\break}
\newcommand{\lemma}[1]{\par\noindent\settowidth{\hangindent}{\textbf{Lemma: }}\textbf{Lemma: }#1\n}
\newcommand{\defn}[1]{\par\noindent\settowidth{\hangindent}{\textbf{Defn: }}\textbf{Defn: }#1\n}
\newcommand{\recap}[1]{\par\noindent\settowidth{\hangindent}{\textbf{Recap: }}\textbf{Recap: }#1\n}
\newcommand{\eg}[1]{\par\noindent\settowidth{\hangindent}{\textbf{E.g.: }}\textbf{E.g.: }#1\n}
\newcommand{\thm}[1]{\par\noindent\settowidth{\hangindent}{\textbf{Thm: }}\textbf{Thm: }#1\n}
\newcommand{\cor}[1]{\par\noindent\settowidth{\hangindent}{\textbf{Cor: }}\textbf{Cor: }#1\n}
\newcommand{\pf}[1]{\par\noindent\settowidth{\hangindent}{\textbf{Proof: }}\textbf{Proof: }#1\n}
\newcommand{\proven}{\;$\square$\n}
\newcommand{\problem}[1]{\par\noindent{#1}\n}
\newcommand{\problempart}[2]{\par\settowidth{\hangindent}{\textbf{(#1)} \indent{}}\textbf{(#1)} #2\n}
\newcommand{\ptxt}[1]{\textrm{\textnormal{#1}}}
\newcommand{\inlineeq}[1]{\n\centerline{$\displaystyle #1$}}
\newcommand{\pageline}{\noindent\rule{\textwidth}{0.1pt}\n}

% Math
\newcommand{\reals}{\mathbb{R}}
\newcommand{\R}{\reals}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\Z}{\integers}
\newcommand{\rationals}{\mathbb{Q}}
\newcommand{\Q}{\rationals}
\newcommand{\dom}{\textbf{dom}\;}
\newcommand{\epi}{\textbf{epi}\;}
\newcommand{\prob}{\textbf{prob}}
\newcommand{\ceil}{\text{ceil}}

% Probability
\newcommand{\F}{\mathcal F} 
\newcommand{\parSymbol}{\P}
\newcommand{\Prob}{\mathbb{P}}
\renewcommand{\P}{\Prob}
\newcommand{\Avg}{\mathbb{E}}
\newcommand{\E}{\Avg}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\Unif}{Unif}
\DeclareMathOperator{\Binom}{Binom}
\newcommand{\CI}{\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10mu\perp$}}}}

% Text
\newcommand{\tand}{\;\text{and}\;}

\title{Lecture 10}
\author{Professor Virginia R. Young\\ \small{Transcribed by Hao Chen}}
\date{September 26, 2022}

\begin{document}

\maketitle

\subsection*{Expectation of a r.v.}

\defn{The expectation of $X:\text{real-valued r.v.}$ equals
\[\E X=\int^\infty_{-\infty} x\;dF(x)\]
if it exists
\\\\
Aside: Riemann-Stieltjes integral
\begin{align*}
    &\int^a_b f(x)\;dg(x) \\
    =&\lim_{\|\pi\|\rightarrow0}\sum^n_{i=1}f(x_i^*)(g(x_i)-g(x_{i-1}))
\end{align*}
where the norm of partition $\pi$ 
\[\|\pi\|=max|x_i-x_{i-1}|, \qquad\text{if }b=\infty\]
\\\\
Back to expectation:
\begin{align*}
    \E X&=\int^\infty_{-\infty} x\;dF(x) \\
    &=\int^\infty_{-\infty}\int^x_0 1\;dt\;dF(x) \\
    &=\int^{-\infty}_0\int^t_{-\infty} 1\;dF(x)\;dt + \int^\infty_{0}\int^\infty_t 1\;dF(x)\;dt \\
    &=\int^{-\infty}_0 F(t)\;dt+\int^\infty_0 (1-F(t))\;dt \\
    &=-\int_{-\infty}^0 F(t)\;dt+\int^\infty_0 (1-F(t))\;dt
\end{align*}
}

\defn{Expectation for a Discrete r.v.
\\\\
$\implies$ $F$ is a step function and only jumps at discrete points $x_1, x_2, \dots$. 
\\\\
$\implies$
\begin{align*}
    \E X&=\int^\infty_{-\infty} x\;dF(x) \\
    &=\sum^\infty_{k=1}x_k(F(x_k)-F(x_k^-)) \\
    &=\sum^\infty_{k=1}x_k\P(X=x_k)
\end{align*}
}

\eg{$X\sim\text{Bernoulli}(p)$, $0<p<1$
\[\P(X=k)=\left\{\begin{array}{lc}1-p&k=0\\p&k=1\end{array}\right.\]
\[\E X=0\cdot(1-p)+1\cdot p=p\]
}

\eg{$X\sim\text{Binomial(n,p)}$
$X\sim X_1+\dots+X_n$, $X_i$ are independent $\text{Bernoulli}(p)$.
\\\\
Later, we will show that $\E$ is a linear operator, meaning
\[\E(aX+bY)=a\E X+b\E Y\]
where $a, b\in\R$, $X, Y$ are r.v.s.
\[\therefore\E(X)=n\E(X_i)=np\]
\[\P(X=k)=\begin{pmatrix}n\\k\end{pmatrix}p^k(1-p)^{n-k}, \quad k=0,1,2,\dots,n\]
\begin{align*}
    \E X&=\sum^n_{k=0}k\cdot\begin{pmatrix}n\\k\end{pmatrix}p^k(1-p)^{n-k} \\
    &=\sum^n_{k=1}k\cdot\frac{n!}{k!(n-k)!}\cdot p^k(1-p)^{n-k} \\
    &=np\cdot\sum^n_{k=1}\cdot\frac{(n-1)!}{(k-1)!(n-k)!}\cdot p^{k-1}(1-p)^{n-k} \\
    &=np\cdot\sum^{n-1}_{j=0}\cdot\frac{(n-1)!}{j!((n-1)-j)!}\cdot p^j(1-p)^{(n-1)-j}, \qquad\text{For $j=k-1$}\\
    &=np
\end{align*}
The sum $\sum^{n-1}_{j=0}\cdot\frac{(n-1)!}{j!((n-1)-j)!}\cdot p^j(1-p)^{(n-1)-j} = 1$ because it's the pmf of $\text{Binomial}(n-1,p)$.
%TODO

}


\eg{$X\sim\text{Geometric(p)}$
\[\P(X=k)=\P(X=k)=(1-p)^{k-1}\cdot p,\quad k=1, 2, \dots\]
\begin{align*}
    \E X&=\sum^\infty_{k=1}k\cdot(1-p)^{k-1}\cdot p \\
    &=-p\sum^\infty_{k=1}\frac{d}{dp}(1-p)^k \\
    &=-p\frac{d}{dp}\left(\sum^\infty_{k=1}(1-p)^k\right) \\
    &=-p\frac{d}{dp}\frac{(1-p)-0}{1-(1-p)} \\
    &=-p\frac{d}{dp}(\frac{1-p}{p}) \\
    &=-p\frac{d}{dp}(\frac{1}{p}-1) =\frac{-p}{-p^2} =\frac{1}{p}
\end{align*}
Intuitively, \[p=\frac{\text{success}}{\text{trial}},\qquad\E X=\frac{\text{trial}}{\text{success}}=\frac{1}{p}\]
}

\eg{$X\sim\text{Poisson}(\lambda)$
\[\P(X=k)=e^{-\lambda}\cdot\frac{\lambda^k}{k!},\quad k=0, 1, 2, \dots\]
\begin{align*}
    \E X&=\sum^\infty_{k=0}k\cdot e^{-\lambda}\cdot\frac{\lambda^k}{k!} \\
    &=\lambda\sum^\infty_{k=1} e^{-\lambda}\cdot\frac{\lambda^{k-1}}{(k-1)!} \\
    &=\lambda\sum^\infty_{j=0} e^{-\lambda}\cdot\frac{\lambda^j}{j!} \\
    &=\lambda
\end{align*}
Ad hoc pf that
\[\text{Binomial}(n,p)\xrightarrow{n\rightarrow\infty}\text{Poisson}(\lambda)\]
For $np=\lambda$,
\[\lim_{n\rightarrow\infty}\E(\text{Binomial}(n,p))=\E(\text{Poisson}(\lambda))\]
}

\defn{Expectation for a Continuous r.v. \\
$X$ = continuous r.v. means $dF(x)=f(x)\;dx$ where $f(x)$ is pdf
\[\E X=\int^\infty_{-\infty}x\;dF(x)=\int^\infty_{-\infty}xf(x)\;dx\]
}

\eg{$X\sim U(a, b)$
\[f(x)=\frac{1}{b-a}, \qquad a<x<b\]
\begin{align*}
    \E X&=\int^b_a\frac{x}{b-a}\;dx \\
    &=\frac{b^2-a^2}{2(b-a)}=\frac{b+a}{2}
\end{align*}
}

\end{document}
